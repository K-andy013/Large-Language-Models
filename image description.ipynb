{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"collapsed":true,"id":"r7UgO4J4nNRD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n","Requirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers\u003c0.21,\u003e=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.10)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.2.3)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.8.30)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch) (3.0.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1+cu121)\n","Requirement already satisfied: pillow!=8.3.*,\u003e=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1-\u003etorchvision) (3.16.1)\n","Requirement already satisfied: typing-extensions\u003e=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1-\u003etorchvision) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1-\u003etorchvision) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1-\u003etorchvision) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1-\u003etorchvision) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1-\u003etorchvision) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch==2.5.1-\u003etorchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch==2.5.1-\u003etorchvision) (3.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n"]}],"source":["!pip install transformers\n","!pip install torch\n","!pip install torchvision\n","!pip install pillow\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fUjbLd04Vtp2"},"outputs":[],"source":["import torch\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from PIL import Image\n","import requests\n","from io import BytesIO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"60oo0JFFVzxB"},"outputs":[],"source":["images=[\"https://th.bing.com/th?id=OIP.M-bg1y9cFEEt8RpkafzOTwHaFj\u0026w=288\u0026h=216\u0026c=8\u0026rs=1\u0026qlt=90\u0026o=6\u0026dpr=1.5\u0026pid=3.1\u0026rm=2\",\n","        \"https://thumbs.dreamstime.com/z/kwame-nkrumah-memorial-park-monument-knmp-national-accra-ghana-named-osagyefo-dr-48650280.jpg\",\n","        \"https://www.zdnet.com/a/img/resize/49b1ec66e176430592d81ce6aa37aa499bfb4abb/2023/09/13/750409b0-0d4e-4bc9-90f2-16565bae042c/iphone-15-pro-max-white-1.jpg?auto=webp\u0026precrop=2048,1151,x0,y12\u0026width=1280\",\n","        \"https://th.bing.com/th/id/OIP.R02cmN97jMvlUAboM9BPJQHaE5?rs=1\u0026pid=ImgDetMain\",\n","        \"http://wondrlust.com/wp-content/uploads/2015/10/happiness-960x540.jpg\",\n","        \"https://www.tastefullyoffensive.com/wp-content/uploads/2021/01/1bf4f15dbcb6f8923a59f60eae-confusing-pictures.jpg\",\n","        \"https://www.anopticalillusion.com/wp-content/uploads/2020/10/duck-rabbit-optical-illusion.png\",\n","        \"https://img.freepik.com/free-photo/world-cuisine-with-delicious-food_23-2151890014.jpg?t=st=1733420813~exp=1733424413~hmac=9a6c3be25c7aa82fa6bbf5c6eccc0f520234c50294940cd266653a7d1c185c8b\u0026w=1060\",\n","        \"https://th.bing.com/th/id/OIP.jSfJczEBk-t89yYiEaJ7QwHaFC?w=227\u0026h=180\u0026c=7\u0026r=0\u0026o=5\u0026dpr=2\u0026pid=1.7\",\n","        \"https://www.soccerbible.com/media/117952/balls-6-min.jpg\"\n","\n","        ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"s94cpkiEcAi_"},"outputs":[{"data":{"text/plain":["10"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["len(images)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"C57dSOUVmRAj"},"outputs":[],"source":["\n","\n","\n","# Function to generate image description\n","def generate_image_description(image_url):\n","    # Load the BLIP model and processor\n","    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","\n","    # Download the image\n","    response = requests.get(image_url)\n","    img = Image.open(BytesIO(response.content)).convert('RGB')\n","\n","    # Display the image\n","    display(img)\n","\n","    # Prepare the image for the model\n","    inputs = processor(images=img, return_tensors=\"pt\")\n","\n","    # Generate description\n","    out = model.generate(**inputs)\n","    description = processor.decode(out[0], skip_special_tokens=True)\n","\n","    return description\n","\n","# Example usage\n","# image_url = 'https://example.com/path_to_your_image.jpg'\n","#description = generate_image_description(image_url)\n","#print(\"Generated Description:\")\n","#print(description)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1xbcJmbdeLym2KtC6niZ_kDqN-mIa8tfI"},"collapsed":true,"id":"a-wWCv-fdenN","outputId":"346a1246-87c6-4d24-a1ec-68629af0a831"},"outputs":[],"source":["for i in images:\n","  print(generate_image_description(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"collapsed":true,"id":"laitBW2UGqrr"},"outputs":[],"source":["from transformers import AutoProcessor, AutoModelForCausalLM\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R39fGIueGYdP"},"outputs":[],"source":["from transformers import logging\n","logging.set_verbosity_error()\n","\n","\n","\n","# Function to generate image description\n","def generate_image_description_git(image_url):\n","    # Load the BLIP model and processor\n","    processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textcaps\")\n","    model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textcaps\")\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    # Download the image\n","    response = requests.get(image_url)\n","    img = Image.open(BytesIO(response.content)).convert('RGB')\n","\n","    # Display the image\n","    display(img)\n","\n","    # Prepare the image for the model\n","    inputs = processor(images=img, return_tensors=\"pt\")\n","   # pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n","\n","    # Generate description\n","    pixel_values = inputs.pixel_values.to(device)\n","\n","    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n","\n","    return(\"Generated caption:\", processor.batch_decode(generated_ids, skip_special_tokens=True))\n","\n","\n","# Example usage\n","# image_url = 'https://example.com/path_to_your_image.jpg'\n","#description = generate_image_description(image_url)\n","#print(\"Generated Description:\")\n","#print(description)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1iF6wTnkjQKtF735G6A4N3MSZ_iFVwt4g"},"id":"fzlDXTadMEWR","outputId":"3b716891-150b-40f6-afa9-c36f45b605d9"},"outputs":[],"source":["for i in images:\n","  print(generate_image_description_git(i))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZSqbF4NVJsdN"},"outputs":[],"source":["import torch\n","from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n","\n","\n","# Function to generate image description\n","def generate_image_description_gpt2(image_url):\n","    # Load model components\n","    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","    feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    max_length = 16\n","    num_beams = 4\n","    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n","\n","    # Download the image\n","    response = requests.get(image_url)\n","    img = Image.open(BytesIO(response.content)).convert('RGB')\n","\n","    # Display the image\n","    display(img)\n","\n","    # Prepare the image for the model\n","    pixel_values = feature_extractor(images=img, return_tensors=\"pt\").pixel_values\n","    pixel_values = pixel_values.to(device)\n","\n"," # Generate predictions\n","    output_ids = model.generate(pixel_values, **gen_kwargs)\n","\n","    # Decode the prediction and return the caption\n","    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n","    return caption\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1VZ533L9gW8rmY8VAdADFHc3tohcprPb5"},"collapsed":true,"id":"39tPVYc-R7jv","outputId":"7e346cb8-b7da-45fa-8646-28be1d328bf9"},"outputs":[],"source":["for i in images:\n","  print(generate_image_description_gpt2(i))"]},{"cell_type":"markdown","metadata":{"id":"xlMKLnBy0jRQ"},"source":["Picture 1(A man and woman walking down the street| OK\n","A man and a woman are standing in front of a woman.| GOOD\n","a man and a woman standing next to each other| OK\n",")  \n","Picture 2(Statue of buddha in the temple of the buddha| FALSE\n","A statue of a man pointing to the right with a blue background.| GOOD\n","A statue of a man sitting on top of a stone wall|OK\n",")\n","Picture 3(The iphone 11 is seen in this photo taken from the back of the phone 11| GOOD\n","A phone with the back of a phone that has a lens on it| GOOD\n","a white remote control sitting on top of a white table|FALSE\n","\n",")\n","Picture 4(A diagram of the plant's life|FALSE\n","A picture of photosynthesis with photosynthesis| GOOD\n","a vase filled with flowers on top of a table| BAD\n",")\n","Picture 5(A woman in a yellow shirt holding an umbrella|OK\n","A girl with her arms outstretched with her arms outstretched in a field with a multicolored umbrella.|GOOD\n","a woman is flying a colorful kite in the sky| FALSE\n","\n","\n","\n",")\n","Picture 6(A man kneeling down to pet a horse| GOOD\n","A man kneeling down next to a horse in a field|GOOD\n","a brown horse standing on top of a lush green hillside| GOOD\n","\n",")\n","Picture 7(A black and white drawing of a duck|OK\n","A black and white drawing of a bird with a black and white background.|GOOD\n","a statue of a bird on a wall|FALSE\n","\n",")\n","Picture 8(A pizza with pepperoni and cheese|GOOD\n","A pizza with pepperoni and cheese on it sits on a table.|BETTER\n","a pizza sitting on top of a metal pan|OK\n","\n",")\n","Picture 9(A toy car sitting on the ground|GOOD\n","'A red car with a fire extinguisher on the side.|OK\n","a red and white double decker bus parked on a sidewalk|BAD\n","\n",")\n","Picture 10(A soccer ball on the grass|GOOD\n","A soccer ball with a picture of a horse on it|BETTER\n","A soccer ball on a field with grass|GOOD\n",")"]},{"cell_type":"markdown","metadata":{"id":"3waBqQrC8_5a"},"source":[]},{"cell_type":"markdown","metadata":{"id":"rnRY0NvV9F6T"},"source":["BLIP\n","Microsoft GIT\n","VIT-GPT2"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","provenance":[{"file_id":"1Ub8MQlle8m8OefAP2JMFH2_Sa5fGzEkV","timestamp":1733673778342}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}